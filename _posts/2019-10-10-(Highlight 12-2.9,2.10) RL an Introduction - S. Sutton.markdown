---
layout: post
title: 'Highlight12 - 2.9 Associative Search (Contextual Bandits) <br> 2.10 Summary <br> [RL an Introduction - S. Sutton]'
date: 2019-10-10
categories: machine_learning
image: images/RLIntroduction_Sutton.png
---
### 2.9 Associative Search (Contextual Bandits)
So far in this chapter we have considered only nonassociative tasks, that is, tasks in which there is no need to associate different actions with different situations.
<br>

However, in a general reinforcement learning task there is more than one situation.
<br>

Suppose that when a bandit task is selected for you, you are given some distinctive clue about its identity(but not its action values).
<br>

This is an example of an associative search task, so called because it involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best. Associative search tasks are often now called contextual bandits in the literature.
<br>

If actions are allowed to affect the next situation as well as the reward, then we have the full reinforcement learning problem.

### 2.10 Summary
We have presented in this chapter several simple ways of balancing exploration and exploitation. The "$\epsilon$-greedy methods choose randomly a small fraction of the time, whereas UCB methods choose deterministically but achieve exploration by subtly favoring at each step the actions that have so far received fewer samples. Gradient bandit algorithms estimate not action values, but action preferences, and favor the more preferred actions in a graded, probabilistic manner using a soft-max distribution.
<br>

The following shows this measure for the various bandit algorithms from this chapter, each as a function of its own parameter shown on a single scale on the x-axis. This kind of graph is called a parameter study.

![Figure2.6](/images/Figure2.6.png){: .image-center}

Despite their simplicity, in our opinion the methods presented in this chapter can fairly be considered the state of the art. There are more sophisticated methods, but their complexity and assumptions make them impractical for the full reinforcement learning problem that is our real focus.






<br>
\-----------------

# Reinforcement Learning an Introduction - Richard S. Sutton

### I Tabular Solution Methods
#### 2 Multi-armed Bandits
2.1 A k-armed Bandit Problem<br>
2.2 Action-value Methods<br>
2.3 The 10-armed Testbed<br>
2.4 Incremental Implementation<br>
2.5 Tracking a Nonstationary Problem<br>
2.6 Optimistic Initial Values<br>
2.7 Upper-Confidence-Bound Action Selection<br>
2.8 Gradient Bandit Algorithms<br>
<b>2.9 Associative Search (Contextual Bandits)<br>
2.10 Summary<br></b>
