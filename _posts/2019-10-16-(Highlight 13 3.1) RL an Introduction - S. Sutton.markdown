---
layout: post
title: 'Highlight13 - 3.1 The Agent-Environment Interface <br> [RL an Introduction - S. Sutton]'
date: 2019-10-16
categories: machine_learning
image: images/RLIntroduction_Sutton.png
---
## Chapter 3 Finite Markov Decision
In this chapter we introduce the formal problem of finite Markov decision process, or finite MDPs. MDPs are a classical formalization of sequential decision making, where actions influence not just immediate rewards, but also subsequent situations, or state, and through those future rewards. In MDPs we estimate the value $ q_{\ast}(s,a) $ of each action $a$ in each state $s$, or we estimate the value $v_{\ast}(s)$ of each state given optimal action selections.

### 3.1 the Agent-Environment Interface
The learner and decision maker is called the agent. the thing it interacts with, comprising everything outside the agent, is called the environment.
<br>

The environment gives rise to rewards that the agent seeks to maximize over time through its choice of actions.

![Figure3.1](/images/Figure3.1.png){: .image-center}

At each step $t$, the agent receives the environment's state, $S_t \in \mathcal{S}$, and on the basis selects and action, $A_t \in \mathcal{A}(s)$. One step later, the agent receives a numerical reward, $R_{t+1} \in \mathcal{R} \subset \Bbb{R}$, and finds itself in a new state, $S_{t+1}$. The MDP and agent together thereby give rise to a sequence or trajectory that begins like this:

$$
S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ...
$$

In a finite MDP, the random variables $R_t$ and $S_t$ have well defined discrete probability distributions dependent only on the preceding state and action.
<br>

For particular values of these random variables, $s' \in \mathcal{S}$ and $r \in \mathcal{R}$, there is a probability of those values occurring at time $t$, given particular values of the preceding state and action:

$$
p(s',r|s,a) \doteq Pr\{S_t = s', R_t = r | S_{t-1} = s, A_{t-1} = a\},
$$

for all $s', s \in \mathcal{S}, r \in \mathcal{R},\text{ and } a \in \mathcal{A}(s)$. The function $p$ defines the dynamics of the MDP.
<br>

$p$ specifies a probability distribution for each choice of $s$ and $a$, that is, that

$$
\sum_{s'\in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s',r|s,a) = 1, \text{    for all } s \in \mathcal{S}, a \in \mathcal{A}(s)
$$

In Markov decision process, the porbabilities given by $p$ completely characterize the environment's dynamics.
<br>

The state must include information about all aspects of the past agent-environment interaction that ake a difference for the future. If it does, the state is said to have the Markov property.
<br>

The state-transition probabilities,

$$
p(s'|s,a) \doteq Pr\{S_t = s' | S_{t-1} = s, A_{t-1} = a\} = \sum_{r \in \mathcal{R}} p(s',r|s,a)
$$

The expected rewards for state-action pair:

$$
r(s,a) \doteq \Bbb{E}[R_t | S_{t-1} = s, A_{t-1} = a] = \sum_{r \in \mathcal{R}} r \sum_{s'\in \mathcal{S}} p(s',r|s,a)
$$

The expected rewards for state-action-next-state triples:

$$
p(s,a,s') \doteq \Bbb{E}[R_t | S_{t-1} = s, A_{t-1} = a, S_t = s'] = \sum_{r \in \mathcal{R}} r \frac{p(s',r|s,a)}{p(s'|s,a)} .
$$

The MDP framework is abstract and flexible and can be applied to many different problems in many different ways.
<br>

In general, actions can be any decisioins we want to learn how to make, and the states can be anything we can know that might be useful in making them.
<br>

The boundary between agent and environment is drawn closer to the agent than that.
<br>

The general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outsied of it and thus part of its environment. We do not assume that everything in the environment is unknown to the agent.
<br>

In fact, the agent may know everything about how its environment works and still face a difficult reinforcement learning task.
<br>

The agent-environment boundary represents the limit of the agent's absolute control, not of its knowledge.
<br>

The agent-environment boundary can be located at different places for different purposes.
<br>

The MDP framework is a considerable abstraction of the problem of goal-directed learning from interaction.
<br>

This framework may not be sufficient to represent all decision-learning problems usefully, but it has proved to be widely useful and applicable.






<br>
\-----------------

# Reinforcement Learning an Introduction - Richard S. Sutton

### I Tabular Solution Methods
#### 3 Finite Markov Decision Processes
<b>3.1 The Agentâ€“Environment Interface<br></b>
3.2 Goals and Rewards<br>
3.3 Returns and Episodes<br>
3.4 Unified Notation for Episodic and Continuing Tasks<br>
3.5 Policies and Value Functions<br>
3.6 Optimal Policies and Optimal Value Functions<br>
3.7 Optimality and Approximation<br>
3.8 Summary<br>
