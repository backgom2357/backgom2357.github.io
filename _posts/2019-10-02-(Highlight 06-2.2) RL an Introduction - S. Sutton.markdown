---
layout: post
title: 'Highlight06 - 2.2 Action-value Methods <br> [RL an Introduction - S. Sutton]'
date: 2019-10-02
categories: machine_learning
image: images/RLIntroduction_Sutton.png
---
### 2.2 Action-value Methods
We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods. Recall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received:

$$
Q_t(a) \dotep \frac{\text{sum of rewards when a taken prior to t}}{number of times a taken prior to t}
\eq \frac{\sum_{i=1}^{t-1} R_i \cdot \Bbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \Bbb{1}_{A_i=a}}
$$

where $\Bbb{1}_predicate$ denotes the random variable that is 1 if predicate is true and 0 if it is not. If the denominator is zero, then we instead define $Q_t(a)$ as some default value, such as 0. As the denominator goes to infinity, by the law of large numbers, $Q_t(a)$ converges to $q_*(a)$. We call this the sample-average method for estimating action values because each estimate is an average of the sample of relevant rewards.
<br>

The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as defined in the previous section.
<br>

We write this greedy action selection method as

$$

A_t \dotep
$$




<br>
\-----------------

# Reinforcement Learning an Introduction - Richard S. Sutton

### I Tabular Solution Methods
#### 2 Multi-armed Bandits
2.1 A k-armed Bandit Problem<br>
<b>2.2 Action-value Methods<br></b>
2.3 The 10-armed Testbed<br>
2.4 Incremental Implementation<br>
2.5 Tracking a Nonstationary Problem<br>
2.6 Optimistic Initial Values<br>
2.7 Upper-Confidence-Bound Action Selection<br>
2.8 Gradient Bandit Algorithms<br>
2.9 Associative Search (Contextual Bandits)<br>
2.10 Summary<br>
